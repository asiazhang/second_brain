# 代码采集改进实践

@(个人博客)[程序设计, 算法学习]

## 架构改进

### 原有架构
分为组件如下：
1. `code-data-provider`: 提供API对外访问
2. `gerrit-listener`: 监听gerrit事件并解析Change相关数据保存到数据库
3. `git-listener`: 监听codeclub/gitlab仓库事件并解析MR相关数据保存到数据库
4. `code-worker`: 监听push事件消息，并解析代码
5. `code-schduler`:定时任务，检查git仓库状态，并发起代码采集流程

老的架构图：

[![BFJJ8s.jpg](https://s1.ax1x.com/2020/10/22/BFJJ8s.jpg)](https://imgchr.com/i/BFJJ8s)

### 当前架构
组件：
1. `code-v2-provider`: 提供API对外访问
2. `code-v2-listener`: 监听codeclub/gitlab/gerrit事件，并发送到消息队列，不再解析代码相关数据
3. `code-v2-collector`: 监听codeclub/gitlab/gerrit初始化事件和一般事件，负责代码/MR/检视/审核数据的采集
4. `code-v2-schduler`: 定时任务，定期发起初始化事件和重试失败的一般事件

当前架构图:

[![BFYEZT.jpg](https://s1.ax1x.com/2020/10/22/BFYEZT.jpg)](https://imgchr.com/i/BFYEZT)

最主要的改动是将采集数据的业务都放入`code-v2-collector`中，职责更为清晰。组件之前通过消息进行交互，更容易扩展。

### 组件化
新版本将2个重要的模块进行提取，变为二方库。

- `code-parser`: 用于解析本地Git仓库commits信息，提取自原`code-worker`。
- `code-service`: 用于封装跟Gitlab/Gerrit/Codehub/CodeClub的API，对外提供标准的API接口，简化调用。提取自原`gerrit-listener`/`git-listener`/`code-schduler`。

提取出这两个二方库后，现在code-v2组件中代码就变的较少，容易维护。

#### code-parser
解析仓库的commits和对应的change信息，代码量：
- `kotlin`: `1903`行代码，包含空行`279`，注释行`247`，代码行`1377`。
- `yaml`: `6443`行代码，主要是语言配置信息。

包含解析代码和测试代码，测试覆盖率`91%`。其中使用`stream.parallel`来进行并发代码解析，提升了解析速度，相比单线程模式性能提升2~5倍。

#### code-servce
封装代码仓API，提供仓库信息/MergeRequest/检视意见/Githook等信息的查询。代码量：
- `kotlin`: `4390`行代码，包含空行`747`，注释行`116`，代码行`3527`。

包含业务逻辑代码和测试代码，测试覆盖率`89%`。

### 可维护性
代码量减少：

[![BRyfTx.jpg](https://s1.ax1x.com/2020/11/05/BRyfTx.jpg)](https://imgchr.com/i/BRyfTx)

## 可靠性改进
### 事务解析并写入
采集端commits数据解析和写入不是事务的。之前的逻辑是：
1. 解析出commit，保存到代码库
2. 再解析此commit的LOC信息，分析commit详细变更

这样把解析逻辑分成了2步，并且没有增加事务写入，在有些场景（比如版本升级/健康检查失败等程序重启场景）下，可能导致第二步没执行完成，导致commit的代码变更量是null。

当前新版本中，解析出commit的同时也解析此commit的LOC信息，因此入库的时候是一起入库的，不会出现部分成功、部分失败的情况。

### 事务写入数据库
使用[[使用Jooq中的一些心得体会#事务写入|Jooq事务方式写入数据库中]] ，确保commits的相关数据一次性写入，避免出现数据不一致问题。

```kotlin
dslContext.transcation { configration ->
  // 写入AAA数据
  DSL.using(configration).batchInsertIgnore(...)
  // 写入BBB数据
  DSL.using(configration).batchInsertIgnore(...)
  // 写入CCC数据
  DSL.using(configration).batchInsertIgnore(...)
}
```

### 数据库限制
在模型上每个表都增加了[[外键约束]]和[[唯一索引]]。我们的业务场景是采集git仓库的commits数据，然后对其进行分析建模，最终在页面上展示用户和仓库代码活动统计。此业务场景是典型的读多写少的场景，增加外键并不会对性能造成太大影响。

写入数据的时候，我们会使用[[数据库upsert操作|PG数据库]]的特性，跳过重复的数据。

```kotlin
import org.jooq.InsertReturningStep
import org.jooq.InsertSetStep
import org.jooq.Record
import org.jooq.UniqueKey

fun <R: Record> InsertSetStep<R>.batchInsertIgnore(batchList: List<R>, uniqueKey: UniqueKey<R>): InsertReturningStep<R>? {
    return if (batchList.isEmpty()) {
        null
    } else  {
        this.set(batchList[0])
            .also { step ->
                batchList.drop(1)
                    .forEach { step.newRecord().set(it)}
            }
            .onConflictOnConstraint(uniqueKey)
            .doNothing()
    }
}
```

注意代码中使用的`onConflictOnConstraint`方法，这个是核心。`doNothing`表示我们遇到约束冲突的时候，什么都不做，跳过重复数据的插入。

![[外键约束]]

## 性能改进
我们使用了一些方式来改进采集性能。

### 批量写入
之前的版本是解析一个commit就写入数据库，当前新版本修改为解析最多500个commit来批量入库。经过测试性能提升了大概11~15倍。

在[[使用Jooq中的一些心得体会#批量写入|jooq中使用批量写入]]相当简单，用batchInsert即可。

### 并行解析
`code-parser`二方库中，使用Java的[[Java中的并行编程#parallelStream|parallelStream]]开启并发解析来提升速度。

```kotlin
fun batchAnalyze(chunk: List<String>, parallel: Boolean) {
    chunk.let { if (parallel) it.parallelStream() else it.stream() }
        .forEach {
            RevWalk(repo).use { revWalk ->
                parseChange(revWalk)
            }
        }
}
```

可以看到，这里我们使用了`parallelStream`来并发解析。**注意并发解析中，需要确保使用的变量是线程安全的**。之前的版本使用了共享的[[RevWalk]]，导致偶发性错误。后续修改为每次解析使用单独的`RevWalk`，就解决了这个问题。

由于并行解析不方便调试，因此增加了一个`parallel`参数，用于关闭并行解析。这样在单元测试中就比较方便使用了。

### 二进制文件判断优化

在改进code-parser代码时，发现里面有大量的时间花费在`isBinaryFile`函数。

函数如下：

```kotlin
fun isBinaryFile(diffs: List<DiffEntry>): Boolean {
    ByteArrayOutputStream().use { os ->
        val diffFormatter = DiffFormatter(os).apply {
            setDiffComparator(RawTextComparator.WS_IGNORE_ALL)
            setRepository(repo)
        }

        diffs.forEach { diffEntry ->
            diffFormatter.format(diffEntry)
            os.toString(StandardCharsets.UTF_8)
                .split("\n")
                .forEach { line ->
                    if (line.startsWith("@@") && line.endsWith("@@")) {
                        return false
                    }
                }
        }
    }

    return true
}
```

这个需要把所有的Diff全部遍历一次，才能查询出来，效率比较低。新的方案修改为读取此文件的前1000个字节，然后根据字符串编码来判断。因为我们开发活动中常见的编码只有：

- `US-ASCII`
- `UTF-8`
- `UTF-16LE`
- `UTF-16BE`
- `UTF-32LE`
- `UTF-32BE`

其他类型的编码我们都可以认为是二进制文件。我们的源代码中基本上不会出现这些类型之外的字符串格式。

## 模型改进

前面的文章已经提过，主要增加了数据外键和唯一约束。

## 经验总结

1. 开发之前定义的数据模型非常重要。如果数据模型存在问题，那么修复的成本会非常高。
2. 合理使用数据库的[[外键约束]]和[[唯一索引]]，特别是这种数据一致性要求很高的场景。
3. 涉及到[[数据一致性问题]]的数据，尽量使用事务写入。
4. 合理控制[[事务写入大小]]和时间，避免影响数据库性能。
5. 合理使用批量写入，以提升性能。
6. 性能优化时不要猜测，尽量使用Profile工具来进行测试。